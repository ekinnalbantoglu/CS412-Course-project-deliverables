{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "!pip uninstall -y scikit-learn\n",
        "!pip install scikit-learn==1.3.1\n",
        "!pip list | grep scikit-learn\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcgQWesnyFb2",
        "outputId": "0c2d8a3e-fff0-4d3f-8eb6-64f5f5174a3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.24.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Found existing installation: scikit-learn 1.3.1\n",
            "Uninstalling scikit-learn-1.3.1:\n",
            "  Successfully uninstalled scikit-learn-1.3.1\n",
            "Collecting scikit-learn==1.3.1\n",
            "  Using cached scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (3.5.0)\n",
            "Using cached scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Installing collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.3.1\n",
            "scikit-learn                       1.3.1\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Collecting scikit-learn<2,>=1.3.2 (from imbalanced-learn)\n",
            "  Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.1\n",
            "    Uninstalling scikit-learn-1.3.1:\n",
            "      Successfully uninstalled scikit-learn-1.3.1\n",
            "Successfully installed scikit-learn-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('output_data.csv')\n",
        "\n",
        "# Check the first few rows to understand the structure\n",
        "import ast\n",
        "print(df.head())\n",
        "# Convert the \"captions\" column from string to list of strings\n",
        "df['captions'] = df['captions'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "\n",
        "# Concatenate the captions into a unified string\n",
        "df['captions_concatenated'] = df['captions'].apply(lambda captions: ' '.join(captions))\n",
        "\n",
        "!pip install emoji\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "import unicodedata\n",
        "# bert word2vec doc2vec\"\n",
        "import emoji, re\n",
        "\n",
        "def preprocess_text_with_emojis(text):\n",
        "    if not text or isinstance(text, float) and np.isnan(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    text = text.replace('#', ' ').replace(\"'\", \" \").replace(\"‚Äô\", \" \").replace(\"\\\\\", \" \").replace(\"‚Äú\", \" \")\n",
        "    text = text.replace(\"/\", \" \").replace(\":\", \" \").replace(\";\", \" \").replace(\"?\", \" \").replace(\"!\", \" \")\n",
        "    text = text.replace(\"(\", \" \").replace(\")\", \" \").replace(\"[\", \" \").replace(\"]\", \" \")\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    text = emoji.demojize(text, delimiters=(\"\", \" \"))  # Convert emojis to descriptive text\n",
        "    text = re.sub(r'[^a-zA-Z\\s:√ºƒü≈ü√ß√∂ƒ±ƒ∞ƒû√ú≈û√á√ñ]', '', text)  # Keep descriptive emoji text\n",
        "    text = text.casefold()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Preprocess 'biography' and 'captions_concatenated' columns\n",
        "df['bio'] = df['biography'].apply(preprocess_text_with_emojis)\n",
        "df['cap'] = df['captions_concatenated'].apply(preprocess_text_with_emojis)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7BEf-QgznJV",
        "outputId": "8deee156-e76b-416c-b618-e9736eb64cbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          username             label  \\\n",
            "0    taskirancemal  Mom and Children   \n",
            "1    tam_kararinda              Food   \n",
            "2         spart4nn              Food   \n",
            "3  sosyalyiyiciler              Food   \n",
            "4  sonaydizdarahad  Mom and Children   \n",
            "\n",
            "                                           biography    category_name  \\\n",
            "0                                     üìçAntalya / Ka≈ü     Entrepreneur   \n",
            "1     Milliyet Pazar\\nKalori Alacaksan Buna Deƒüeceküìö  Kitchen/cooking   \n",
            "2  K√º√ß√ºk ev\\nKamp ve doƒüa hayatƒ±üèï\\nMutfaƒüƒ±mƒ±z doƒü...    Video creator   \n",
            "3  Founder @bitte.izmir\\nIZMIR yemek/seyahat food...              NaN   \n",
            "4  Dƒ∞ZDAR üßø Ahad üßø Daƒühan\\n@d.a.d.kids\\n@dilanpol...    Personal blog   \n",
            "\n",
            "                                            captions  \n",
            "0  ['Ah Merve ah.. #tineco #temizlik', '√áocukken ...  \n",
            "1  ['Faydasƒ±z Deneyler Serisi‚Äônde bu sefer blende...  \n",
            "2  ['K√ºt√ºk ocaƒüƒ±nda et fajita...\\n#nature#dinner#...  \n",
            "3  ['Ke≈üif gibi ke≈üifle geldim!cosi.gastro √áakall...  \n",
            "4  ['Her ≈üeye EYVALLAH ü§ê', 'ü§ç', '10.yƒ±l ‚ù§Ô∏è', '≈ûan...  \n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Show the result\n",
        "print(df[['biography', 'bio', 'captions_concatenated', 'cap']].head())\n",
        "\n",
        "df = df.drop(columns=['biography','captions', 'captions_concatenated'])\n",
        "\n",
        "# Save the processed DataFrame to a new CSV file\n",
        "df.to_csv('train_temizledim_with_stopWords_ama.csv', index=False)\n",
        "time.sleep(10)\n",
        "# Show the first few rows of the modified DataFrame to confirm\n",
        "print(\"head:\\n\",df.head())\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "csv.field_size_limit(10**6)  # Set an appropriate limit\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Get the Turkish stopwords list\n",
        "turkish_stopwords = stopwords.words('turkish')\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\"\n",
        "    # Split text into words\n",
        "    words = text.split()\n",
        "    # Remove stopwords and return the cleaned text\n",
        "    cleaned_text = ' '.join([word for word in words if word not in turkish_stopwords])\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "# Remove stopwords from 'biography_preprocessed' and 'captions_concatenated_preprocessed'\n",
        "df['bio_noSw'] = df['bio'].apply(remove_stopwords)\n",
        "df['cap_noSw'] = df['cap'].apply(remove_stopwords)\n",
        "\n",
        "# Show the original and stopwords-removed columns side by side for comparison\n",
        "print(df[['bio', 'bio_noSw',\n",
        "          'cap', 'cap_noSw']].head())\n",
        "\n",
        "df.to_csv('train_temiz_noSw.csv', index=False)\n",
        "time.sleep(10)\n",
        "\n",
        "\n",
        "df = pd.read_csv('train_temiz_noSw.csv')\n",
        "df['bio_noSw'] = df['bio_noSw'].fillna(\"\").astype(str)\n",
        "df['cap_noSw'] = df['cap_noSw'].fillna(\"\").astype(str)\n",
        "df.drop(columns=['bio', 'cap'], inplace=True)\n",
        "df.rename(columns={'bio_noSw': 'bio', 'cap_noSw': 'cap'}, inplace=True)\n",
        "print(df.head())\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Initialize the vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=3000,\n",
        "    stop_words=None,\n",
        ")\n",
        "\n",
        "\n",
        "# Fit on training data (captions and biographies combined, or separately)\n",
        "tfidf_vectorizer.fit(df['cap'] + df['bio'])  # Fit on combined text\n",
        "\n",
        "# Transform training set\n",
        "captions_tfidf = tfidf_vectorizer.transform(df['cap'])\n",
        "biography_tfidf = tfidf_vectorizer.transform(df['bio'])\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "biography_tfidf_df = pd.DataFrame(biography_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "captions_tfidf_df = pd.DataFrame(captions_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Combine TF-IDF features with original DataFrame\n",
        "df_with_tfidf = pd.concat([df, biography_tfidf_df.add_prefix('bio_'), captions_tfidf_df.add_prefix('cap_')], axis=1)\n",
        "\n",
        "df_with_tfidf.drop(columns=['bio', 'cap'], inplace=True)\n",
        "print(df_with_tfidf.head())\n",
        "\n",
        "#df_with_tfidf.to_csv('5k_train_x_y_no_category.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3WQlfaB33uN",
        "outputId": "cf212250-4408-4816-d416-234fa5ffa7de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           biography  \\\n",
            "0                                     üìçAntalya / Ka≈ü   \n",
            "1     Milliyet Pazar\\nKalori Alacaksan Buna Deƒüeceküìö   \n",
            "2  K√º√ß√ºk ev\\nKamp ve doƒüa hayatƒ±üèï\\nMutfaƒüƒ±mƒ±z doƒü...   \n",
            "3  Founder @bitte.izmir\\nIZMIR yemek/seyahat food...   \n",
            "4  Dƒ∞ZDAR üßø Ahad üßø Daƒühan\\n@d.a.d.kids\\n@dilanpol...   \n",
            "\n",
            "                                                 bio  \\\n",
            "0                         roundpushpin antalya   kas   \n",
            "1  milliyet pazar kalori alacaksan buna degecekbo...   \n",
            "2  kucuk ev kamp ve doga hayatƒ±camping  mutfagƒ±mƒ±...   \n",
            "3  founder bitteizmir izmir yemek seyahat food tr...   \n",
            "4  dizdar nazaramulet  ahad nazaramulet  daghan d...   \n",
            "\n",
            "                               captions_concatenated  \\\n",
            "0  Ah Merve ah.. #tineco #temizlik √áocukken baktƒ±...   \n",
            "1  Faydasƒ±z Deneyler Serisi‚Äônde bu sefer blender‚Äô...   \n",
            "2  K√ºt√ºk ocaƒüƒ±nda et fajita...\\n#nature#dinner#la...   \n",
            "3  Ke≈üif gibi ke≈üifle geldim!cosi.gastro √áakallƒ± ...   \n",
            "4  Her ≈üeye EYVALLAH ü§ê ü§ç 10.yƒ±l ‚ù§Ô∏è ≈ûanlƒ±urfa da E...   \n",
            "\n",
            "                                                 cap  \n",
            "0  ah merve ah  tineco  temizlik cocukken baktƒ±gƒ±...  \n",
            "1  faydasƒ±z deneyler serisi nde bu sefer blender ...  \n",
            "2  kutuk ocagƒ±nda et fajita  nature dinner lake c...  \n",
            "3  kesif gibi kesifle geldim cosigastro cakallƒ± m...  \n",
            "4  her seye eyvallah zippermouthface  whiteheart ...  \n",
            "head:\n",
            "           username             label    category_name  \\\n",
            "0    taskirancemal  Mom and Children     Entrepreneur   \n",
            "1    tam_kararinda              Food  Kitchen/cooking   \n",
            "2         spart4nn              Food    Video creator   \n",
            "3  sosyalyiyiciler              Food              NaN   \n",
            "4  sonaydizdarahad  Mom and Children    Personal blog   \n",
            "\n",
            "                                                 bio  \\\n",
            "0                         roundpushpin antalya   kas   \n",
            "1  milliyet pazar kalori alacaksan buna degecekbo...   \n",
            "2  kucuk ev kamp ve doga hayatƒ±camping  mutfagƒ±mƒ±...   \n",
            "3  founder bitteizmir izmir yemek seyahat food tr...   \n",
            "4  dizdar nazaramulet  ahad nazaramulet  daghan d...   \n",
            "\n",
            "                                                 cap  \n",
            "0  ah merve ah  tineco  temizlik cocukken baktƒ±gƒ±...  \n",
            "1  faydasƒ±z deneyler serisi nde bu sefer blender ...  \n",
            "2  kutuk ocagƒ±nda et fajita  nature dinner lake c...  \n",
            "3  kesif gibi kesifle geldim cosigastro cakallƒ± m...  \n",
            "4  her seye eyvallah zippermouthface  whiteheart ...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 bio  \\\n",
            "0                         roundpushpin antalya   kas   \n",
            "1  milliyet pazar kalori alacaksan buna degecekbo...   \n",
            "2  kucuk ev kamp ve doga hayatƒ±camping  mutfagƒ±mƒ±...   \n",
            "3  founder bitteizmir izmir yemek seyahat food tr...   \n",
            "4  dizdar nazaramulet  ahad nazaramulet  daghan d...   \n",
            "\n",
            "                                            bio_noSw  \\\n",
            "0                           roundpushpin antalya kas   \n",
            "1  milliyet pazar kalori alacaksan buna degecekbooks   \n",
            "2  kucuk ev kamp doga hayatƒ±camping mutfagƒ±mƒ±z do...   \n",
            "3  founder bitteizmir izmir yemek seyahat food tr...   \n",
            "4  dizdar nazaramulet ahad nazaramulet daghan dad...   \n",
            "\n",
            "                                                 cap  \\\n",
            "0  ah merve ah  tineco  temizlik cocukken baktƒ±gƒ±...   \n",
            "1  faydasƒ±z deneyler serisi nde bu sefer blender ...   \n",
            "2  kutuk ocagƒ±nda et fajita  nature dinner lake c...   \n",
            "3  kesif gibi kesifle geldim cosigastro cakallƒ± m...   \n",
            "4  her seye eyvallah zippermouthface  whiteheart ...   \n",
            "\n",
            "                                            cap_noSw  \n",
            "0  ah merve ah tineco temizlik cocukken baktƒ±gƒ±mƒ±...  \n",
            "1  faydasƒ±z deneyler serisi nde sefer blender la ...  \n",
            "2  kutuk ocagƒ±nda et fajita nature dinner lake ch...  \n",
            "3  kesif kesifle geldim cosigastro cakallƒ± meneme...  \n",
            "4  seye eyvallah zippermouthface whiteheart yƒ±l r...  \n",
            "          username             label    category_name  \\\n",
            "0    taskirancemal  Mom and Children     Entrepreneur   \n",
            "1    tam_kararinda              Food  Kitchen/cooking   \n",
            "2         spart4nn              Food    Video creator   \n",
            "3  sosyalyiyiciler              Food              NaN   \n",
            "4  sonaydizdarahad  Mom and Children    Personal blog   \n",
            "\n",
            "                                                 bio  \\\n",
            "0                           roundpushpin antalya kas   \n",
            "1  milliyet pazar kalori alacaksan buna degecekbooks   \n",
            "2  kucuk ev kamp doga hayatƒ±camping mutfagƒ±mƒ±z do...   \n",
            "3  founder bitteizmir izmir yemek seyahat food tr...   \n",
            "4  dizdar nazaramulet ahad nazaramulet daghan dad...   \n",
            "\n",
            "                                                 cap  \n",
            "0  ah merve ah tineco temizlik cocukken baktƒ±gƒ±mƒ±...  \n",
            "1  faydasƒ±z deneyler serisi nde sefer blender la ...  \n",
            "2  kutuk ocagƒ±nda et fajita nature dinner lake ch...  \n",
            "3  kesif kesifle geldim cosigastro cakallƒ± meneme...  \n",
            "4  seye eyvallah zippermouthface whiteheart yƒ±l r...  \n",
            "          username             label    category_name  bio_about  bio_ac  \\\n",
            "0    taskirancemal  Mom and Children     Entrepreneur        0.0     0.0   \n",
            "1    tam_kararinda              Food  Kitchen/cooking        0.0     0.0   \n",
            "2         spart4nn              Food    Video creator        0.0     0.0   \n",
            "3  sosyalyiyiciler              Food              NaN        0.0     0.0   \n",
            "4  sonaydizdarahad  Mom and Children    Personal blog        0.0     0.0   \n",
            "\n",
            "   bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  ...  cap_zirvesi  \\\n",
            "0       0.0       0.0      0.0       0.0         0.0  ...          0.0   \n",
            "1       0.0       0.0      0.0       0.0         0.0  ...          0.0   \n",
            "2       0.0       0.0      0.0       0.0         0.0  ...          0.0   \n",
            "3       0.0       0.0      0.0       0.0         0.0  ...          0.0   \n",
            "4       0.0       0.0      0.0       0.0         0.0  ...          0.0   \n",
            "\n",
            "   cap_ziyaret  cap_ziyaretleri   cap_zor  cap_zorlu  cap_zorunda  cap_zu  \\\n",
            "0     0.000000              0.0  0.092349        0.0     0.019985     0.0   \n",
            "1     0.000000              0.0  0.000000        0.0     0.006939     0.0   \n",
            "2     0.026061              0.0  0.000000        0.0     0.000000     0.0   \n",
            "3     0.000000              0.0  0.033854        0.0     0.000000     0.0   \n",
            "4     0.000000              0.0  0.000000        0.0     0.000000     0.0   \n",
            "\n",
            "     cap_ƒ±n  cap_ƒ±sƒ±k  cap_ƒ±zgara  \n",
            "0  0.020233       0.0    0.000000  \n",
            "1  0.010538       0.0    0.000000  \n",
            "2  0.000000       0.0    0.026278  \n",
            "3  0.000000       0.0    0.048484  \n",
            "4  0.000000       0.0    0.000000  \n",
            "\n",
            "[5 rows x 6003 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#dftf = pd.read_csv('5k_train_x_y_no_category.csv')\n",
        "dftf= df_with_tfidf\n",
        "dftf['label'] = dftf['label'].replace('Health and lifestyle', 'Health and Lifestyle')\n",
        "\n",
        "print(dftf[10:15])\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(dftf[\"label\"])\n",
        "\n",
        "dftf[\"label_encoded\"] = label_encoder.transform(dftf[\"label\"])\n",
        "\n",
        "\n",
        "label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
        "print(\"Mapping of strings to integers:\", label_mapping)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "category_encoder = LabelEncoder()\n",
        "category_encoder.fit(dftf[\"category_name\"])\n",
        "\n",
        "dftf[\"category_name_encoded\"] = category_encoder.transform(dftf[\"category_name\"])\n",
        "\n",
        "\n",
        "category_mapping = dict(zip(category_encoder.classes_, range(len(category_encoder.classes_))))\n",
        "print(\"Mapping of strings to integers:\", category_mapping)\n",
        "\n",
        "dftf = dftf.drop(columns=[\"label\", \"category_name\"])\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(dftf.head())\n",
        "\n",
        "\n",
        "# original_labels = label_encoder.inverse_transform(labels_encoded)\n",
        "# original_category_names = category_encoder.inverse_transform(df[\"category_name_encoded\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJkc2Cs6Yrro",
        "outputId": "86c3699e-b384-4741-da56-fe1098596c40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             username                 label         category_name  bio_about  \\\n",
            "10     ruyabuyuktetik               Fashion         Personal blog        0.0   \n",
            "11         raykakumru  Health and Lifestyle         Public figure        0.0   \n",
            "12         pintipanda                Gaming  Gaming video creator        0.0   \n",
            "13  pinarindepresyonu  Health and Lifestyle       Digital creator        0.0   \n",
            "14         pinarhotic  Health and Lifestyle       Digital creator        0.0   \n",
            "\n",
            "    bio_ac  bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  ...  \\\n",
            "10     0.0       0.0       0.0      0.0       0.0         0.0  ...   \n",
            "11     0.0       0.0       0.0      0.0       0.0         0.0  ...   \n",
            "12     0.0       0.0       0.0      0.0       0.0         0.0  ...   \n",
            "13     0.0       0.0       0.0      0.0       0.0         0.0  ...   \n",
            "14     0.0       0.0       0.0      0.0       0.0         0.0  ...   \n",
            "\n",
            "    cap_zirvesi  cap_ziyaret  cap_ziyaretleri   cap_zor  cap_zorlu  \\\n",
            "10          0.0          0.0              0.0  0.000000    0.00000   \n",
            "11          0.0          0.0              0.0  0.000000    0.00000   \n",
            "12          0.0          0.0              0.0  0.000000    0.00000   \n",
            "13          0.0          0.0              0.0  0.000000    0.00000   \n",
            "14          0.0          0.0              0.0  0.011352    0.01318   \n",
            "\n",
            "    cap_zorunda  cap_zu    cap_ƒ±n  cap_ƒ±sƒ±k  cap_ƒ±zgara  \n",
            "10          0.0     0.0  0.000000  0.000000         0.0  \n",
            "11          0.0     0.0  0.000000  0.000000         0.0  \n",
            "12          0.0     0.0  0.043444  0.000000         0.0  \n",
            "13          0.0     0.0  0.000000  0.000000         0.0  \n",
            "14          0.0     0.0  0.007462  0.026056         0.0  \n",
            "\n",
            "[5 rows x 6003 columns]\n",
            "Mapping of strings to integers: {'Art': 0, 'Entertainment': 1, 'Fashion': 2, 'Food': 3, 'Gaming': 4, 'Health and Lifestyle': 5, 'Mom and Children': 6, 'Sports': 7, 'Tech': 8, 'Travel': 9}\n",
            "Mapping of strings to integers: {'Actor': 0, 'Advertising Agency': 1, 'Advertising/Marketing': 2, 'Aerospace Company': 3, 'Agricultural Service': 4, 'Agriculture': 5, 'Animal Rescue Service': 6, 'Apartment & Condo Building': 7, 'App page': 8, 'Aquarium': 9, 'Architectural Designer': 10, 'Art': 11, 'Art School': 12, 'Artist': 13, 'Arts & Crafts Store': 14, 'Arts & entertainment': 15, 'Athlete': 16, 'Author': 17, 'Automation Service': 18, 'Automotive Body Shop': 19, 'Automotive Dealership': 20, 'Automotive Manufacturer': 21, 'Automotive Parts Store': 22, 'Automotive Service': 23, 'Aviation School': 24, \"Baby & children's clothing store\": 25, 'Baby goods/kids goods': 26, 'Bags/Luggage': 27, 'Bakery': 28, 'Bar': 29, 'Beach Resort': 30, 'Beauty Supply Store': 31, 'Beauty, cosmetic & personal care': 32, 'Bicycle Shop': 33, 'Biotechnology Company': 34, 'Blinds & Curtains Store': 35, 'Blogger': 36, 'Boat Dealership': 37, 'Boat Rental': 38, 'Boat Tour Agency': 39, 'Book Series': 40, 'Bookstore': 41, 'Brand': 42, 'Bridal Shop': 43, 'Broadcasting & media production company': 44, 'Building Materials': 45, 'Burger Restaurant': 46, 'Bus Line': 47, 'Business Center': 48, 'Business Consultant': 49, 'Business service': 50, 'Butcher Shop': 51, 'Cafe': 52, 'Campus Building': 53, 'Canal': 54, 'Car Rental': 55, 'Car dealership': 56, 'Cargo & Freight Company': 57, 'Carpet & Flooring Store': 58, 'Cars': 59, 'Caterer': 60, 'Charity Organization': 61, 'Cheese Shop': 62, 'Chef': 63, 'Chemical Company': 64, 'Child Development': 65, 'Chocolate Shop': 66, 'City Hall': 67, 'Civilization Museum': 68, 'Clothing (Brand)': 69, 'Clothing store': 70, 'Coach': 71, 'Cocktail Bar': 72, 'Coffee shop': 73, 'College & university': 74, 'Comedian': 75, 'Comfort Food Restaurant': 76, 'Comic Bookstore': 77, 'Commercial & Industrial': 78, 'Commercial & Industrial Equipment Supplier': 79, 'Commercial Real Estate Agency': 80, 'Community': 81, 'Community Organization': 82, 'Community Service': 83, 'Company': 84, 'Computer Company': 85, 'Computers & Internet Website': 86, 'Construction Company': 87, 'Consulting agency': 88, 'Cosmetic Dentist': 89, 'Cosmetics store': 90, 'Dairy Farm': 91, 'Dance & Night Club': 92, 'Dance School': 93, 'Defense Company': 94, 'Deli': 95, 'Dentist & Dental Office': 96, 'Design & fashion': 97, 'Designer': 98, 'Dessert Shop': 99, 'Digital creator': 100, 'Doctor': 101, 'Donut Shop': 102, 'E-commerce website': 103, 'Editor': 104, 'Education': 105, 'Education website': 106, 'Educational Consultant': 107, 'Educational Research Center': 108, 'Electric Utility Provider': 109, 'Electronics': 110, 'Electronics Company': 111, 'Electronics Store': 112, 'Elevator Service': 113, 'Energy Company': 114, 'Engineering Service': 115, 'Entrepreneur': 116, 'Event': 117, 'Event Planner': 118, 'Family Style Restaurant': 119, 'Fashion Designer': 120, 'Fast food restaurant': 121, 'Festival': 122, 'Film Director': 123, 'Finance': 124, 'Financial service': 125, 'Fishing Store': 126, 'Florist': 127, 'Food & Drink': 128, 'Food & beverage': 129, 'Food & beverage company': 130, 'Food Consultant': 131, 'Food Wholesaler': 132, 'Footwear store': 133, 'Franchise Broker': 134, 'Furniture': 135, 'Furniture store': 136, 'Games/toys': 137, 'Gaming video creator': 138, 'Gastropub': 139, 'Gift Shop': 140, 'Government Official': 141, 'Government organization': 142, 'Graphic Designer': 143, 'Grocery Store': 144, 'Gym/Physical Fitness Center': 145, 'Hair Replacement Service': 146, 'Hair Salon': 147, 'Harbor': 148, 'Health & wellness website': 149, 'Health Food Restaurant': 150, 'Health Food Store': 151, 'Health/beauty': 152, 'Heating, Ventilating & Air Conditioning Service': 153, 'History Museum': 154, 'Home & Garden Store': 155, 'Home Improvement': 156, 'Home Security Company': 157, 'Home decor': 158, 'Hospital': 159, 'Hospitality Service': 160, 'Hotel': 161, 'Hotel & Lodging': 162, 'Hotel resort': 163, 'Household supplies': 164, 'Ice Cream Shop': 165, 'Industrial Company': 166, 'Information Technology Company': 167, 'Insurance Agent': 168, 'Insurance Broker': 169, 'Insurance company': 170, 'Interest': 171, 'Internet company': 172, 'Internist (Internal Medicine)': 173, 'Italian Restaurant': 174, 'Japanese Restaurant': 175, 'Jewelry & Watches Store': 176, 'Jewelry/watches': 177, 'Journalist': 178, 'Just for fun': 179, 'Kitchen/cooking': 180, 'Landscape Designer': 181, 'Lighting Store': 182, 'Live Music Venue': 183, 'Local & travel website': 184, 'Local business': 185, 'Local service': 186, 'Lounge': 187, 'Machine Shop': 188, 'Magazine': 189, 'Makeup Artist': 190, 'Marina': 191, 'Marketing Agency': 192, 'Media': 193, 'Media/news company': 194, 'Medical & health': 195, 'Medical Equipment Manufacturer': 196, 'Medical Equipment Supplier': 197, 'Medical Lab': 198, 'Medical Service': 199, 'Mediterranean Restaurant': 200, 'Meeting Room': 201, 'Model': 202, 'Motor vehicle company': 203, 'Motorcycle Dealership': 204, 'Movie Theater': 205, 'Museum': 206, 'Music': 207, 'Musician': 208, 'Musician/band': 209, 'National Park': 210, 'Neurologist': 211, 'News & media website': 212, 'News personality': 213, 'Non-Governmental Organization (NGO)': 214, 'Nonprofit organization': 215, 'Nurseries & Gardening Store': 216, 'Nutritionist': 217, 'Obstetrician-Gynecologist (OBGYN)': 218, 'Organization': 219, 'Otolaryngologist (ENT)': 220, 'Outdoor & Sporting Goods Company': 221, 'Outdoor Equipment Store': 222, 'Performance & Event Venue': 223, 'Performance Art': 224, 'Performance Art Theatre': 225, 'Performing Arts': 226, 'Performing Arts School': 227, 'Personal Coach': 228, 'Personal blog': 229, 'Pet Service': 230, 'Pet Supplies': 231, 'Pharmaceutical Company': 232, 'Pharmaceuticals': 233, 'Phone/Tablet': 234, 'Photographer': 235, 'Photography Videography': 236, 'Physical Therapist': 237, 'Pilates Studio': 238, 'Plastic Manufacturer': 239, 'Plastic Surgeon': 240, 'Political Candidate': 241, 'Political Organization': 242, 'Political Party': 243, 'Politician': 244, 'Portable Building Service': 245, 'Preschool': 246, 'Printing Service': 247, 'Private School': 248, 'Producer': 249, 'Product/service': 250, 'Professional Service': 251, 'Professional Sports Team': 252, 'Prosthodontist': 253, 'Psychologist': 254, 'Pub': 255, 'Public & Government Service': 256, 'Public Relations Agency': 257, 'Public Service': 258, 'Public figure': 259, 'Publisher': 260, 'RV Rental': 261, 'Radio station': 262, 'Real Estate': 263, 'Real Estate Company': 264, 'Recycling Center': 265, 'Residence': 266, 'Restaurant': 267, 'Retail company': 268, 'School': 269, 'Science Website': 270, 'Science, Technology & Engineering': 271, 'Scientist': 272, 'Screen Printing & Embroidery': 273, 'Seafood Restaurant': 274, 'Shopping & retail': 275, 'Shopping Mall': 276, 'Shopping Service': 277, 'Show': 278, 'Ski Resort': 279, 'Skin Care Service': 280, 'Social Club': 281, 'Social Media Agency': 282, 'Society & culture website': 283, 'Software': 284, 'Software Company': 285, 'Solar Energy Company': 286, 'Solar Energy Service': 287, 'Soup Restaurant': 288, 'Spa': 289, 'Specialty Grocery Store': 290, 'Sports': 291, 'Sports & recreation': 292, 'Sports Club': 293, 'Sports Event': 294, 'Sports league': 295, 'Sports team': 296, 'Steakhouse': 297, 'Sunglasses & Eyewear Store': 298, 'Surfing Spot': 299, 'Surgical Center': 300, 'TV channel': 301, 'TV show': 302, 'Talent Manager': 303, 'Tattoo & Piercing Shop': 304, 'Telecommunication company': 305, 'Textile Company': 306, 'Theatrical Productions': 307, 'Theme Restaurant': 308, 'Therapist': 309, 'Tire Dealer & Repair Shop': 310, 'Tour Agency': 311, 'Tourist Information Center': 312, 'Toy Store': 313, 'Transit Stop': 314, 'Transportation Service': 315, 'Travel Company': 316, 'Travel agency': 317, 'Turkish Restaurant': 318, 'Urologist': 319, 'Used Vehicles': 320, 'Vegetarian/Vegan Restaurant': 321, 'Video creator': 322, 'Village': 323, 'Visual Arts': 324, 'Vitamins/supplements': 325, 'Web Designer': 326, 'Website': 327, 'Websites & Blogs': 328, 'Wedding Venue': 329, 'Wholesale & Supply Store': 330, 'Writer': 331, 'Youth Organization': 332, nan: 333}\n",
            "          username  bio_about  bio_ac  bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  \\\n",
            "0    taskirancemal        0.0     0.0       0.0       0.0      0.0       0.0   \n",
            "1    tam_kararinda        0.0     0.0       0.0       0.0      0.0       0.0   \n",
            "2         spart4nn        0.0     0.0       0.0       0.0      0.0       0.0   \n",
            "3  sosyalyiyiciler        0.0     0.0       0.0       0.0      0.0       0.0   \n",
            "4  sonaydizdarahad        0.0     0.0       0.0       0.0      0.0       0.0   \n",
            "\n",
            "   bio_acƒ±lan  bio_acƒ±ldƒ±  bio_acƒ±lƒ±s  ...  cap_ziyaretleri   cap_zor  \\\n",
            "0         0.0         0.0         0.0  ...              0.0  0.092349   \n",
            "1         0.0         0.0         0.0  ...              0.0  0.000000   \n",
            "2         0.0         0.0         0.0  ...              0.0  0.000000   \n",
            "3         0.0         0.0         0.0  ...              0.0  0.033854   \n",
            "4         0.0         0.0         0.0  ...              0.0  0.000000   \n",
            "\n",
            "   cap_zorlu  cap_zorunda  cap_zu    cap_ƒ±n  cap_ƒ±sƒ±k  cap_ƒ±zgara  \\\n",
            "0        0.0     0.019985     0.0  0.020233       0.0    0.000000   \n",
            "1        0.0     0.006939     0.0  0.010538       0.0    0.000000   \n",
            "2        0.0     0.000000     0.0  0.000000       0.0    0.026278   \n",
            "3        0.0     0.000000     0.0  0.000000       0.0    0.048484   \n",
            "4        0.0     0.000000     0.0  0.000000       0.0    0.000000   \n",
            "\n",
            "   label_encoded  category_name_encoded  \n",
            "0              6                    116  \n",
            "1              3                    180  \n",
            "2              3                    322  \n",
            "3              3                    333  \n",
            "4              6                    229  \n",
            "\n",
            "[5 rows x 6003 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Isolate the label column\n",
        "labels = dftf['label_encoded']\n",
        "\n",
        "# Perform stratified splitting\n",
        "train_df, test_df = train_test_split(\n",
        "    dftf,\n",
        "    test_size=0.2,  # 20% test size\n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Verify the split\n",
        "label_distribution_train = train_df['label_encoded'].value_counts(normalize=False)\n",
        "label_distribution_test = test_df['label_encoded'].value_counts(normalize=False)\n",
        "\n",
        "print(\"Label distribution in train set:\")\n",
        "print(label_distribution_train)\n",
        "\n",
        "print(\"\\nLabel distribution in test set:\")\n",
        "print(label_distribution_test)\n",
        "\n",
        "# Check the shapes of the resulting datasets\n",
        "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
        "print(f\"Test set shape: {test_df.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch6bliB8-Vc2",
        "outputId": "22ebe18d-1323-45ba-a5ce-6ce197d38dce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution in train set:\n",
            "label_encoded\n",
            "3    409\n",
            "5    402\n",
            "8    277\n",
            "1    258\n",
            "2    239\n",
            "9    235\n",
            "0    153\n",
            "6    119\n",
            "7     90\n",
            "4     10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution in test set:\n",
            "label_encoded\n",
            "3    102\n",
            "5    100\n",
            "8     69\n",
            "1     65\n",
            "2     60\n",
            "9     59\n",
            "0     38\n",
            "6     30\n",
            "7     23\n",
            "4      3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set shape: (2192, 6003)\n",
            "Test set shape: (549, 6003)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#train_df.columns[-11:-1]\n",
        "#train_df.to_csv('5ktrain_df_1hot.csv', index=False)\n",
        "#\n",
        "#test_df.columns[-11:-1]\n",
        "#test_df.to_csv('5ktest_df_1hot.csv', index=False)\n",
        "#\n",
        "#train_df = pd.read_csv('5ktrain_df_1hot.csv')\n",
        "#test_df = pd.read_csv('5ktest_df_1hot.csv')\n",
        "\n",
        "#Step 2: Data Preparation\n",
        "#Drop unnecessary columns:\n",
        "\n",
        "train_X = train_df.drop(columns=['username', 'label_encoded'])\n",
        "test_X = test_df.drop(columns=['username', 'label_encoded'])\n",
        "\n",
        "print(train_X.head())\n",
        "# Extract the one-hot-encoded labels for training and testing\n",
        "train_y = train_df.iloc[:,-2]  # Last 11 columns for one-hot-encoded labels\n",
        "test_y = test_df.iloc[:,-2]\n",
        "\n",
        "print(train_X.head())\n",
        "print(test_X.head())\n",
        "print(train_y.head())\n",
        "print(test_y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Uzh0zHDSzv",
        "outputId": "cb39a10b-be6d-46d3-be1b-b62c481c56c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      bio_about  bio_ac  bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  \\\n",
            "2600        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "335         0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "2157        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "292         0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "1544        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "\n",
            "      bio_acƒ±ldƒ±  bio_acƒ±lƒ±s  bio_acƒ±sƒ±ndan  ...  cap_ziyaret  \\\n",
            "2600         0.0         0.0            0.0  ...     0.000000   \n",
            "335          0.0         0.0            0.0  ...     0.037989   \n",
            "2157         0.0         0.0            0.0  ...     0.000000   \n",
            "292          0.0         0.0            0.0  ...     0.000000   \n",
            "1544         0.0         0.0            0.0  ...     0.000000   \n",
            "\n",
            "      cap_ziyaretleri  cap_zor  cap_zorlu  cap_zorunda  cap_zu  cap_ƒ±n  \\\n",
            "2600              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "335               0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "2157              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "292               0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "1544              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "\n",
            "      cap_ƒ±sƒ±k  cap_ƒ±zgara  category_name_encoded  \n",
            "2600       0.0         0.0                     29  \n",
            "335        0.0         0.0                      2  \n",
            "2157       0.0         0.0                    152  \n",
            "292        0.0         0.0                    275  \n",
            "1544       0.0         0.0                    316  \n",
            "\n",
            "[5 rows x 6001 columns]\n",
            "      bio_about  bio_ac  bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  \\\n",
            "2600        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "335         0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "2157        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "292         0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "1544        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "\n",
            "      bio_acƒ±ldƒ±  bio_acƒ±lƒ±s  bio_acƒ±sƒ±ndan  ...  cap_ziyaret  \\\n",
            "2600         0.0         0.0            0.0  ...     0.000000   \n",
            "335          0.0         0.0            0.0  ...     0.037989   \n",
            "2157         0.0         0.0            0.0  ...     0.000000   \n",
            "292          0.0         0.0            0.0  ...     0.000000   \n",
            "1544         0.0         0.0            0.0  ...     0.000000   \n",
            "\n",
            "      cap_ziyaretleri  cap_zor  cap_zorlu  cap_zorunda  cap_zu  cap_ƒ±n  \\\n",
            "2600              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "335               0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "2157              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "292               0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "1544              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "\n",
            "      cap_ƒ±sƒ±k  cap_ƒ±zgara  category_name_encoded  \n",
            "2600       0.0         0.0                     29  \n",
            "335        0.0         0.0                      2  \n",
            "2157       0.0         0.0                    152  \n",
            "292        0.0         0.0                    275  \n",
            "1544       0.0         0.0                    316  \n",
            "\n",
            "[5 rows x 6001 columns]\n",
            "      bio_about  bio_ac  bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  \\\n",
            "1338        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "2723        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "1056        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "2423        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "2312        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "\n",
            "      bio_acƒ±ldƒ±  bio_acƒ±lƒ±s  bio_acƒ±sƒ±ndan  ...  cap_ziyaret  \\\n",
            "1338         0.0         0.0            0.0  ...     0.000000   \n",
            "2723         0.0         0.0            0.0  ...     0.000000   \n",
            "1056         0.0         0.0            0.0  ...     0.000000   \n",
            "2423         0.0         0.0            0.0  ...     0.050543   \n",
            "2312         0.0         0.0            0.0  ...     0.104765   \n",
            "\n",
            "      cap_ziyaretleri  cap_zor  cap_zorlu  cap_zorunda  cap_zu  cap_ƒ±n  \\\n",
            "1338              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "2723              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "1056              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "2423              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "2312              0.0      0.0        0.0          0.0     0.0     0.0   \n",
            "\n",
            "      cap_ƒ±sƒ±k  cap_ƒ±zgara  category_name_encoded  \n",
            "1338       0.0         0.0                     13  \n",
            "2723       0.0         0.0                     81  \n",
            "1056       0.0         0.0                    100  \n",
            "2423       0.0         0.0                    250  \n",
            "2312       0.0         0.0                    275  \n",
            "\n",
            "[5 rows x 6001 columns]\n",
            "2600    3\n",
            "335     1\n",
            "2157    5\n",
            "292     8\n",
            "1544    9\n",
            "Name: label_encoded, dtype: int64\n",
            "1338    0\n",
            "2723    3\n",
            "1056    9\n",
            "2423    8\n",
            "2312    9\n",
            "Name: label_encoded, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjXzngzVhOWD",
        "outputId": "6ed46ff8-9d42-413b-b84a-4197a5b50fe7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6651480637813212\n",
            "Test Accuracy: 0.6484517304189436\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.39      0.45        38\n",
            "           1       0.42      0.38      0.40        65\n",
            "           2       0.74      0.67      0.70        60\n",
            "           3       0.81      0.86      0.84       102\n",
            "           4       0.00      0.00      0.00         3\n",
            "           5       0.55      0.80      0.65       100\n",
            "           6       0.57      0.43      0.49        30\n",
            "           7       0.82      0.39      0.53        23\n",
            "           8       0.68      0.75      0.72        69\n",
            "           9       0.81      0.58      0.67        59\n",
            "\n",
            "    accuracy                           0.65       549\n",
            "   macro avg       0.59      0.53      0.55       549\n",
            "weighted avg       0.66      0.65      0.64       549\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42,shuffle=False)\n",
        "\n",
        "\n",
        "best_params = {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300, 'subsample': 0.7,\n",
        "               'colsample_bytree': 1.0, 'gamma': 0.1, 'reg_alpha': 0, 'reg_lambda': 0.1}\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "xgb_model = XGBClassifier(\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    subsample=best_params['subsample'],\n",
        "    colsample_bytree=best_params['colsample_bytree'],\n",
        "    gamma=best_params['gamma'],\n",
        "    reg_alpha=best_params['reg_alpha'],\n",
        "    reg_lambda=best_params['reg_lambda'],\n",
        "    #tree_method='hist',\n",
        "    #device='cuda',\n",
        "    eval_metric='mlogloss',\n",
        "    objective='multi:softmax', # For multi-class classification\n",
        "    num_class=len(np.unique(train_y)), # Number of classes\n",
        "    random_state=42,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Define the parameter grid to search\n",
        "''' param_grid = {\n",
        "    'max_depth': [4],             # Depth of trees\n",
        "    'learning_rate': [0.01,0.1],  # Step size shrinkage\n",
        "    'n_estimators': [200,300],     # Number of trees\n",
        "   'subsample': [0.7],  # Fraction of samples for each tree\n",
        "   'colsample_bytree': [1.0],# Fraction of features for each tree 7\n",
        "   'gamma': [0.1],             # Minimum loss reduction for split\n",
        "   'reg_alpha': [0],           # L1 regularization\n",
        "   'reg_lambda': [0.1]           # L2 regularization\n",
        "} '''\n",
        "\n",
        "# Set up GridSearchCV with cross-validation\n",
        "#grid_search = GridSearchCV(\n",
        "#    estimator=xgb_model,\n",
        "#    param_grid=param_grid,\n",
        "#    scoring='accuracy',\n",
        "#    cv=3,                # Number of cross-validation folds\n",
        "#    verbose=2,\n",
        "#    n_jobs=1\n",
        "#)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "#grid_search.fit(train_X, train_y)\n",
        "#print(\"Best hyperparameters found: \", grid_search.best_params_)\n",
        "#\n",
        "## Get the best model from GridSearchCV\n",
        "#best_model = grid_search.best_estimator_\n",
        "#import matplotlib.pyplot as plt\n",
        "#from xgboost import plot_importance\n",
        "#\n",
        "##plot_importance(xgb_model)\n",
        "##plt.show()\n",
        "## Evaluate the model with cross-validation\n",
        "#from sklearn.model_selection import cross_val_score\n",
        "#cv_scores = cross_val_score(xgb_model, train_X, train_y, cv=5, scoring='accuracy')\n",
        "#print(f\"Cross-validation scores: {cv_scores}\")\n",
        "#print(f\"Average cross-validation accuracy: {np.mean(cv_scores)}\")\n",
        "#\n",
        "## Predict on test set\n",
        "#y_pred = best_model.predict(X_val)\n",
        "#validation_accuracy = accuracy_score(y_val, y_pred)\n",
        "#print(f\"Validation Accuracy: {validation_accuracy}\")\n",
        "#\n",
        "## Evaluate the best model on the test set\n",
        "#y_test_pred = best_model.predict(test_X)\n",
        "#test_accuracy = accuracy_score(test_y, y_test_pred)\n",
        "#print(f\"Test Accuracy: {test_accuracy}\")\n",
        "#\n",
        "#print(\"\\nClassification Report:\")\n",
        "#print(classification_report(test_y, y_test_pred))\n",
        "#joblib.dump(xgb_model, 'xgb_model.pkl')\n",
        "\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Accuracy: {validation_accuracy}\")\n",
        "#\n",
        "## Evaluate the model on the test set\n",
        "y_test_pred = xgb_model.predict(test_X)\n",
        "test_accuracy = accuracy_score(test_y, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "#\n",
        "## Print classification report to assess performance\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_y, y_test_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Fs3VbHytPnNO",
        "outputId": "3b7db508-a884-43d3-8023-89b63fe4756d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      bio_about  bio_ac  bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  \\\n",
              "1338        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "2723        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "1056        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "2423        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "2312        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "...         ...     ...       ...       ...      ...       ...         ...   \n",
              "350         0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "1546        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "1751        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "1514        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "2716        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
              "\n",
              "      bio_acƒ±ldƒ±  bio_acƒ±lƒ±s  bio_acƒ±sƒ±ndan  ...  cap_ziyaret  \\\n",
              "1338         0.0         0.0            0.0  ...     0.000000   \n",
              "2723         0.0         0.0            0.0  ...     0.000000   \n",
              "1056         0.0         0.0            0.0  ...     0.000000   \n",
              "2423         0.0         0.0            0.0  ...     0.050543   \n",
              "2312         0.0         0.0            0.0  ...     0.104765   \n",
              "...          ...         ...            ...  ...          ...   \n",
              "350          0.0         0.0            0.0  ...     0.000000   \n",
              "1546         0.0         0.0            0.0  ...     0.000000   \n",
              "1751         0.0         0.0            0.0  ...     0.000000   \n",
              "1514         0.0         0.0            0.0  ...     0.012109   \n",
              "2716         0.0         0.0            0.0  ...     0.000000   \n",
              "\n",
              "      cap_ziyaretleri   cap_zor  cap_zorlu  cap_zorunda  cap_zu    cap_ƒ±n  \\\n",
              "1338              0.0  0.000000   0.000000     0.000000     0.0  0.000000   \n",
              "2723              0.0  0.000000   0.000000     0.000000     0.0  0.000000   \n",
              "1056              0.0  0.000000   0.000000     0.000000     0.0  0.000000   \n",
              "2423              0.0  0.000000   0.000000     0.000000     0.0  0.000000   \n",
              "2312              0.0  0.000000   0.000000     0.000000     0.0  0.000000   \n",
              "...               ...       ...        ...          ...     ...       ...   \n",
              "350               0.0  0.000000   0.000000     0.000000     0.0  0.000000   \n",
              "1546              0.0  0.000000   0.000000     0.000000     0.0  0.006421   \n",
              "1751              0.0  0.000000   0.000000     0.000000     0.0  0.000000   \n",
              "1514              0.0  0.017050   0.019796     0.000000     0.0  0.078448   \n",
              "2716              0.0  0.073127   0.084901     0.094951     0.0  0.000000   \n",
              "\n",
              "      cap_ƒ±sƒ±k  cap_ƒ±zgara  category_name_encoded  \n",
              "1338       0.0         0.0                     13  \n",
              "2723       0.0         0.0                     81  \n",
              "1056       0.0         0.0                    100  \n",
              "2423       0.0         0.0                    250  \n",
              "2312       0.0         0.0                    275  \n",
              "...        ...         ...                    ...  \n",
              "350        0.0         0.0                    192  \n",
              "1546       0.0         0.0                    264  \n",
              "1751       0.0         0.0                    152  \n",
              "1514       0.0         0.0                    260  \n",
              "2716       0.0         0.0                    331  \n",
              "\n",
              "[549 rows x 6001 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22d2b7e7-f365-4ea4-8d4f-02f2a08a6bf1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bio_about</th>\n",
              "      <th>bio_ac</th>\n",
              "      <th>bio_acar</th>\n",
              "      <th>bio_acil</th>\n",
              "      <th>bio_acƒ±</th>\n",
              "      <th>bio_acƒ±k</th>\n",
              "      <th>bio_acƒ±lan</th>\n",
              "      <th>bio_acƒ±ldƒ±</th>\n",
              "      <th>bio_acƒ±lƒ±s</th>\n",
              "      <th>bio_acƒ±sƒ±ndan</th>\n",
              "      <th>...</th>\n",
              "      <th>cap_ziyaret</th>\n",
              "      <th>cap_ziyaretleri</th>\n",
              "      <th>cap_zor</th>\n",
              "      <th>cap_zorlu</th>\n",
              "      <th>cap_zorunda</th>\n",
              "      <th>cap_zu</th>\n",
              "      <th>cap_ƒ±n</th>\n",
              "      <th>cap_ƒ±sƒ±k</th>\n",
              "      <th>cap_ƒ±zgara</th>\n",
              "      <th>category_name_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1338</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2723</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2423</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050543</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2312</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.104765</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1546</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006421</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1751</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1514</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012109</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017050</td>\n",
              "      <td>0.019796</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.078448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2716</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073127</td>\n",
              "      <td>0.084901</td>\n",
              "      <td>0.094951</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>549 rows √ó 6001 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22d2b7e7-f365-4ea4-8d4f-02f2a08a6bf1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22d2b7e7-f365-4ea4-8d4f-02f2a08a6bf1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22d2b7e7-f365-4ea4-8d4f-02f2a08a6bf1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9a3ccd3c-512c-49c7-a490-646a5ff99b60\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9a3ccd3c-512c-49c7-a490-646a5ff99b60')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9a3ccd3c-512c-49c7-a490-646a5ff99b60 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_06f9ba89-ad16-4ab5-a976-c08fee65c2f3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_X')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_06f9ba89-ad16-4ab5-a976-c08fee65c2f3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_X');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_X"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import joblib\n",
        "joblib.dump(xgb_model, 'xgboost_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0cU9RqUwh3o",
        "outputId": "b289c463-ac29-4af8-e387-e28a596ce87f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xgboost_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing with p2 data\n",
        "df = pd.read_csv('test_datap3.csv')\n",
        "print(df.head())\n",
        "df['captions'] = df['captions'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "\n",
        "# Concatenate the captions into a unified string\n",
        "df['captions_concatenated'] = df['captions'].apply(lambda captions: ' '.join(captions))\n",
        "\n",
        "# Preprocess 'biography' and 'captions_concatenated' columns\n",
        "df['bio'] = df['biography'].apply(preprocess_text_with_emojis)\n",
        "df['cap'] = df['captions_concatenated'].apply(preprocess_text_with_emojis)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3f9YixOKfUN",
        "outputId": "dec7ed2a-55b7-43be-e5ad-c683984afab0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        username                                          biography  \\\n",
            "0  livapastanesi  √áukurambar/Farabi/A.City/Yƒ±ldƒ±z/Ayrancƒ±/Taurus...   \n",
            "1     barisgross  %100 Yerli Besi GuÃàvenilir Et\\nTaptaze Meyve v...   \n",
            "2      tusasshop  T√ºrk Havacƒ±lƒ±k Uzay Sanayii Lisanslƒ± UÃàruÃànler...   \n",
            "3  etolyadigital  #G√ºc√ºn√ºKe≈üfet\\nMarkalarƒ± geli≈ütiren dijital pa...   \n",
            "4     tugrulonur  Genel Yayƒ±n Y√∂netmeni ve Moderat√∂r @voleapp \\n...   \n",
            "\n",
            "      category_name                                           captions  \n",
            "0      Dessert Shop  ['\" Atat√ºrk, bir lider olmanƒ±n √ßok daha √∂tesin...  \n",
            "1    Retail company  ['Halk G√ºn√º Fƒ±rsatlarƒ±nƒ± Ka√ßƒ±rma !\\nüçÖü•¨üå∂Ô∏èüçãü´ëüåΩü•ëüçë\\...  \n",
            "2   Product/service  ['Hi√ß durmadan, hi√ß yorulmadan, daima g√∂sterdi...  \n",
            "3  Marketing Agency  ['Devamlƒ±lƒ±k ba≈üarƒ±yƒ± getirir. üí™\\n\\n#webajans ...  \n",
            "4        Journalist  ['Hull City‚Äôye uƒüurlu geldim. Takƒ±m, √ºst√ºn oyn...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Show the result\n",
        "print(df[['biography', 'bio', 'captions_concatenated', 'cap']].head())\n",
        "\n",
        "df = df.drop(columns=['biography','captions', 'captions_concatenated'])\n",
        "print(\"head:\\n\",df.head())\n",
        "\n",
        "\n",
        "csv.field_size_limit(10**6)  # Set an appropriate limit\n",
        "\n",
        "# Remove stopwords from 'biography_preprocessed' and 'captions_concatenated_preprocessed'\n",
        "df['bio_noSw'] = df['bio'].apply(remove_stopwords)\n",
        "df['cap_noSw'] = df['cap'].apply(remove_stopwords)\n",
        "\n",
        "# Show the original and stopwords-removed columns side by side for comparison\n",
        "print(df[['bio', 'bio_noSw',\n",
        "          'cap', 'cap_noSw']].head())\n",
        "\n",
        "\n",
        "df['bio_noSw'] = df['bio_noSw'].fillna(\"\").astype(str)\n",
        "df['cap_noSw'] = df['cap_noSw'].fillna(\"\").astype(str)\n",
        "df.drop(columns=['bio', 'cap'], inplace=True)\n",
        "df.rename(columns={'bio_noSw': 'bio', 'cap_noSw': 'cap'}, inplace=True)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Transform test set using the same vectorizer\n",
        "captions_tfidf_test = tfidf_vectorizer.transform(df['cap'])\n",
        "biography_tfidf_test = tfidf_vectorizer.transform(df['bio'])\n",
        "\n",
        "biography_tfidf_test = pd.DataFrame(biography_tfidf_test.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "captions_tfidf_test = pd.DataFrame(captions_tfidf_test.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "\n",
        "# Combine TF-IDF features with original DataFrame\n",
        "df_with_tfidf = pd.concat([df, biography_tfidf_test.add_prefix('bio_'), captions_tfidf_test.add_prefix('cap_')], axis=1)\n",
        "\n",
        "df_with_tfidf.drop(columns=['bio', 'cap'], inplace=True)\n",
        "print(df_with_tfidf.head())\n",
        "\n",
        "dftf= df_with_tfidf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjbCkBCKPBJQ",
        "outputId": "8506deb8-ef5b-44ff-b5b2-3ddf4d4a9f4c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           biography  \\\n",
            "0  √áukurambar/Farabi/A.City/Yƒ±ldƒ±z/Ayrancƒ±/Taurus...   \n",
            "1  %100 Yerli Besi GuÃàvenilir Et\\nTaptaze Meyve v...   \n",
            "2  T√ºrk Havacƒ±lƒ±k Uzay Sanayii Lisanslƒ± UÃàruÃànler...   \n",
            "3  #G√ºc√ºn√ºKe≈üfet\\nMarkalarƒ± geli≈ütiren dijital pa...   \n",
            "4  Genel Yayƒ±n Y√∂netmeni ve Moderat√∂r @voleapp \\n...   \n",
            "\n",
            "                                                 bio  \\\n",
            "0  cukurambar farabi acity yƒ±ldƒ±z ayrancƒ± taurus ...   \n",
            "1   yerli besi guvenilir et taptaze meyve ve sebz...   \n",
            "2  turk havacƒ±lƒ±k uzay sanayii lisanslƒ± urunleri ...   \n",
            "3   gucunukesfet markalarƒ± gelistiren dijital paz...   \n",
            "4  genel yayƒ±n yonetmeni ve moderator voleapp  pa...   \n",
            "\n",
            "                               captions_concatenated  \\\n",
            "0  \" Atat√ºrk, bir lider olmanƒ±n √ßok daha √∂tesinde...   \n",
            "1  Halk G√ºn√º Fƒ±rsatlarƒ±nƒ± Ka√ßƒ±rma !\\nüçÖü•¨üå∂Ô∏èüçãü´ëüåΩü•ëüçë\\n\\...   \n",
            "2  Hi√ß durmadan, hi√ß yorulmadan, daima g√∂sterdiƒüi...   \n",
            "3  Devamlƒ±lƒ±k ba≈üarƒ±yƒ± getirir. üí™\\n\\n#webajans #s...   \n",
            "4  Hull City‚Äôye uƒüurlu geldim. Takƒ±m, √ºst√ºn oynad...   \n",
            "\n",
            "                                                 cap  \n",
            "0   ataturk bir lider olmanƒ±n cok daha otesindeyd...  \n",
            "1  halk gunu fƒ±rsatlarƒ±nƒ± kacƒ±rma   tomato leafyg...  \n",
            "2  hic durmadan hic yorulmadan daima gosterdigi h...  \n",
            "3  devamlƒ±lƒ±k basarƒ±yƒ± getirir flexedbiceps    we...  \n",
            "4  hull city ye ugurlu geldim takƒ±m ustun oynadƒ±g...  \n",
            "head:\n",
            "         username     category_name  \\\n",
            "0  livapastanesi      Dessert Shop   \n",
            "1     barisgross    Retail company   \n",
            "2      tusasshop   Product/service   \n",
            "3  etolyadigital  Marketing Agency   \n",
            "4     tugrulonur        Journalist   \n",
            "\n",
            "                                                 bio  \\\n",
            "0  cukurambar farabi acity yƒ±ldƒ±z ayrancƒ± taurus ...   \n",
            "1   yerli besi guvenilir et taptaze meyve ve sebz...   \n",
            "2  turk havacƒ±lƒ±k uzay sanayii lisanslƒ± urunleri ...   \n",
            "3   gucunukesfet markalarƒ± gelistiren dijital paz...   \n",
            "4  genel yayƒ±n yonetmeni ve moderator voleapp  pa...   \n",
            "\n",
            "                                                 cap  \n",
            "0   ataturk bir lider olmanƒ±n cok daha otesindeyd...  \n",
            "1  halk gunu fƒ±rsatlarƒ±nƒ± kacƒ±rma   tomato leafyg...  \n",
            "2  hic durmadan hic yorulmadan daima gosterdigi h...  \n",
            "3  devamlƒ±lƒ±k basarƒ±yƒ± getirir flexedbiceps    we...  \n",
            "4  hull city ye ugurlu geldim takƒ±m ustun oynadƒ±g...  \n",
            "                                                 bio  \\\n",
            "0  cukurambar farabi acity yƒ±ldƒ±z ayrancƒ± taurus ...   \n",
            "1   yerli besi guvenilir et taptaze meyve ve sebz...   \n",
            "2  turk havacƒ±lƒ±k uzay sanayii lisanslƒ± urunleri ...   \n",
            "3   gucunukesfet markalarƒ± gelistiren dijital paz...   \n",
            "4  genel yayƒ±n yonetmeni ve moderator voleapp  pa...   \n",
            "\n",
            "                                            bio_noSw  \\\n",
            "0  cukurambar farabi acity yƒ±ldƒ±z ayrancƒ± taurus ...   \n",
            "1  yerli besi guvenilir et taptaze meyve sebzeler...   \n",
            "2  turk havacƒ±lƒ±k uzay sanayii lisanslƒ± urunleri ...   \n",
            "3  gucunukesfet markalarƒ± gelistiren dijital paza...   \n",
            "4  genel yayƒ±n yonetmeni moderator voleapp part t...   \n",
            "\n",
            "                                                 cap  \\\n",
            "0   ataturk bir lider olmanƒ±n cok daha otesindeyd...   \n",
            "1  halk gunu fƒ±rsatlarƒ±nƒ± kacƒ±rma   tomato leafyg...   \n",
            "2  hic durmadan hic yorulmadan daima gosterdigi h...   \n",
            "3  devamlƒ±lƒ±k basarƒ±yƒ± getirir flexedbiceps    we...   \n",
            "4  hull city ye ugurlu geldim takƒ±m ustun oynadƒ±g...   \n",
            "\n",
            "                                            cap_noSw  \n",
            "0  ataturk bir lider olmanƒ±n cok otesindeydi vizy...  \n",
            "1  halk gunu fƒ±rsatlarƒ±nƒ± kacƒ±rma tomato leafygre...  \n",
            "2  hic durmadan hic yorulmadan daima gosterdigi h...  \n",
            "3  devamlƒ±lƒ±k basarƒ±yƒ± getirir flexedbiceps webaj...  \n",
            "4  hull city ye ugurlu geldim takƒ±m ustun oynadƒ±g...  \n",
            "        username     category_name  \\\n",
            "0  livapastanesi      Dessert Shop   \n",
            "1     barisgross    Retail company   \n",
            "2      tusasshop   Product/service   \n",
            "3  etolyadigital  Marketing Agency   \n",
            "4     tugrulonur        Journalist   \n",
            "\n",
            "                                                 bio  \\\n",
            "0  cukurambar farabi acity yƒ±ldƒ±z ayrancƒ± taurus ...   \n",
            "1  yerli besi guvenilir et taptaze meyve sebzeler...   \n",
            "2  turk havacƒ±lƒ±k uzay sanayii lisanslƒ± urunleri ...   \n",
            "3  gucunukesfet markalarƒ± gelistiren dijital paza...   \n",
            "4  genel yayƒ±n yonetmeni moderator voleapp part t...   \n",
            "\n",
            "                                                 cap  \n",
            "0  ataturk bir lider olmanƒ±n cok otesindeydi vizy...  \n",
            "1  halk gunu fƒ±rsatlarƒ±nƒ± kacƒ±rma tomato leafygre...  \n",
            "2  hic durmadan hic yorulmadan daima gosterdigi h...  \n",
            "3  devamlƒ±lƒ±k basarƒ±yƒ± getirir flexedbiceps webaj...  \n",
            "4  hull city ye ugurlu geldim takƒ±m ustun oynadƒ±g...  \n",
            "        username     category_name  bio_about  bio_ac  bio_acar  bio_acil  \\\n",
            "0  livapastanesi      Dessert Shop        0.0     0.0       0.0       0.0   \n",
            "1     barisgross    Retail company        0.0     0.0       0.0       0.0   \n",
            "2      tusasshop   Product/service        0.0     0.0       0.0       0.0   \n",
            "3  etolyadigital  Marketing Agency        0.0     0.0       0.0       0.0   \n",
            "4     tugrulonur        Journalist        0.0     0.0       0.0       0.0   \n",
            "\n",
            "   bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  bio_acƒ±ldƒ±  ...  cap_zirvesi  cap_ziyaret  \\\n",
            "0      0.0       0.0         0.0         0.0  ...          0.0     0.000000   \n",
            "1      0.0       0.0         0.0         0.0  ...          0.0     0.004467   \n",
            "2      0.0       0.0         0.0         0.0  ...          0.0     0.166013   \n",
            "3      0.0       0.0         0.0         0.0  ...          0.0     0.010079   \n",
            "4      0.0       0.0         0.0         0.0  ...          0.0     0.000000   \n",
            "\n",
            "   cap_ziyaretleri  cap_zor  cap_zorlu  cap_zorunda  cap_zu    cap_ƒ±n  \\\n",
            "0              0.0      0.0   0.000000          0.0     0.0  0.000000   \n",
            "1              0.0      0.0   0.000000          0.0     0.0  0.000000   \n",
            "2              0.0      0.0   0.000000          0.0     0.0  0.000000   \n",
            "3              0.0      0.0   0.000000          0.0     0.0  0.009328   \n",
            "4              0.0      0.0   0.027904          0.0     0.0  0.031595   \n",
            "\n",
            "   cap_ƒ±sƒ±k  cap_ƒ±zgara  \n",
            "0  0.000000         0.0  \n",
            "1  0.000000         0.0  \n",
            "2  0.000000         0.0  \n",
            "3  0.000000         0.0  \n",
            "4  0.027582         0.0  \n",
            "\n",
            "[5 rows x 6002 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"check\\n\",dftf['category_name'].head())\n",
        "\n",
        "# Check for unseen categories\n",
        "unseen_categories = set(dftf[\"category_name\"]) - set(category_encoder.classes_)\n",
        "\n",
        "# Add unseen categories to encoder with a special label (333)\n",
        "if unseen_categories:\n",
        "    print(f\"Unseen categories: {unseen_categories}\")\n",
        "    all_categories = list(category_encoder.classes_)\n",
        "    all_categories.append(\"UNSEEN_CATEGORY\")  # Placeholder for unseen categories\n",
        "    category_encoder.classes_ = np.array(all_categories)\n",
        "\n",
        "# Map unseen categories to 333\n",
        "dftf[\"category_name_encoded\"] = dftf[\"category_name\"].apply(\n",
        "    lambda x: 333 if x in unseen_categories else category_encoder.transform([x])[0]\n",
        ")\n",
        "\n",
        "#333 is the NaN mapping\n",
        "\n",
        "dftf = dftf.drop(columns=[\"category_name\"])\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(dftf['category_name_encoded'].head())\n",
        "# original_labels = label_encoder.inverse_transform(labels_encoded)\n",
        "# original_category_names = category_encoder.inverse_transform(df[\"category_name_encoded\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPrzMERHeXUT",
        "outputId": "4404c7ba-bb9b-4ea6-81d8-64dfe21d9e1b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check\n",
            " 0        Dessert Shop\n",
            "1      Retail company\n",
            "2     Product/service\n",
            "3    Marketing Agency\n",
            "4          Journalist\n",
            "Name: category_name, dtype: object\n",
            "Unseen categories: {nan, 'Hobby Store', 'Tour Guide', 'Cafeteria', 'Travel Service', 'Professional Networking', 'Lawyer & Law Firm', 'Dorm', 'Gamer', 'Esports Team', 'Media Agency', 'Podcast', 'Newspaper', 'Cultural Center', 'Pizza place', 'Dance Studio', 'Mobile Phone Shop', 'Glass Manufacturer', 'Interior Design Studio', 'Supermarket', 'Water Park', 'Home Window Service', 'Mining Company', 'Library', 'Entertainment website', 'Fitness Trainer', 'Plastic Fabricator', 'Art Gallery', 'Mental health service', 'Financial Consultant', 'Metal Supplier', 'Book', 'Music Chart', 'Big Box Retailer', 'Glass Blower', 'Movie/television studio', 'Travel & Transportation', 'Boutique Store'}\n",
            "0     99\n",
            "1    268\n",
            "2    250\n",
            "3    192\n",
            "4    178\n",
            "Name: category_name_encoded, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "usernames_col = dftf['username']\n",
        "dftf.drop(columns=['username'], inplace=True)\n",
        "print(dftf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q15QzklxQSLU",
        "outputId": "26bd3f80-891e-4f56-f907-e84d3f92ee2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     bio_about  bio_ac  bio_acar  bio_acil  bio_acƒ±  bio_acƒ±k  bio_acƒ±lan  \\\n",
            "0          0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "1          0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "2          0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "3          0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "4          0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "..         ...     ...       ...       ...      ...       ...         ...   \n",
            "995        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "996        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "997        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "998        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "999        0.0     0.0       0.0       0.0      0.0       0.0         0.0   \n",
            "\n",
            "     bio_acƒ±ldƒ±  bio_acƒ±lƒ±s  bio_acƒ±sƒ±ndan  ...  cap_ziyaret  cap_ziyaretleri  \\\n",
            "0           0.0         0.0            0.0  ...     0.000000              0.0   \n",
            "1           0.0         0.0            0.0  ...     0.004467              0.0   \n",
            "2           0.0         0.0            0.0  ...     0.166013              0.0   \n",
            "3           0.0         0.0            0.0  ...     0.010079              0.0   \n",
            "4           0.0         0.0            0.0  ...     0.000000              0.0   \n",
            "..          ...         ...            ...  ...          ...              ...   \n",
            "995         0.0         0.0            0.0  ...     0.000000              0.0   \n",
            "996         0.0         0.0            0.0  ...     0.000000              0.0   \n",
            "997         0.0         0.0            0.0  ...     0.030972              0.0   \n",
            "998         0.0         0.0            0.0  ...     0.021452              0.0   \n",
            "999         0.0         0.0            0.0  ...     0.000000              0.0   \n",
            "\n",
            "      cap_zor  cap_zorlu  cap_zorunda  cap_zu    cap_ƒ±n  cap_ƒ±sƒ±k  cap_ƒ±zgara  \\\n",
            "0    0.000000   0.000000     0.000000     0.0  0.000000  0.000000         0.0   \n",
            "1    0.000000   0.000000     0.000000     0.0  0.000000  0.000000         0.0   \n",
            "2    0.000000   0.000000     0.000000     0.0  0.000000  0.000000         0.0   \n",
            "3    0.000000   0.000000     0.000000     0.0  0.009328  0.000000         0.0   \n",
            "4    0.000000   0.027904     0.000000     0.0  0.031595  0.027582         0.0   \n",
            "..        ...        ...          ...     ...       ...       ...         ...   \n",
            "995  0.000000   0.000000     0.000000     0.0  0.000000  0.000000         0.0   \n",
            "996  0.051394   0.019890     0.044488     0.0  0.011260  0.000000         0.0   \n",
            "997  0.006230   0.021701     0.000000     0.0  0.028666  0.000000         0.0   \n",
            "998  0.000000   0.000000     0.000000     0.0  0.019855  0.000000         0.0   \n",
            "999  0.022776   0.000000     0.000000     0.0  0.014971  0.000000         0.0   \n",
            "\n",
            "     category_name_encoded  \n",
            "0                       99  \n",
            "1                      268  \n",
            "2                      250  \n",
            "3                      192  \n",
            "4                      178  \n",
            "..                     ...  \n",
            "995                      0  \n",
            "996                    100  \n",
            "997                    333  \n",
            "998                    333  \n",
            "999                     88  \n",
            "\n",
            "[1000 rows x 6001 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = xgb_model.predict(dftf)"
      ],
      "metadata": {
        "id": "qK-ZcSJ2QaeA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HFhL6Q1jKSs",
        "outputId": "272c6eb3-333c-43d1-de88-4997422426b9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Food' 'Food' 'Travel' 'Tech' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Mom and Children' 'Fashion' 'Travel' 'Food' 'Travel'\n",
            " 'Health and Lifestyle' 'Art' 'Fashion' 'Food' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Mom and Children' 'Fashion'\n",
            " 'Food' 'Art' 'Entertainment' 'Fashion' 'Entertainment' 'Art' 'Travel'\n",
            " 'Food' 'Entertainment' 'Art' 'Art' 'Food' 'Health and Lifestyle' 'Art'\n",
            " 'Fashion' 'Entertainment' 'Travel' 'Food' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Mom and Children' 'Tech' 'Entertainment'\n",
            " 'Entertainment' 'Tech' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Food' 'Fashion' 'Mom and Children'\n",
            " 'Health and Lifestyle' 'Fashion' 'Mom and Children' 'Travel' 'Fashion'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Fashion' 'Health and Lifestyle' 'Tech' 'Food' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Fashion' 'Entertainment' 'Fashion' 'Tech' 'Tech'\n",
            " 'Food' 'Fashion' 'Health and Lifestyle' 'Fashion' 'Health and Lifestyle'\n",
            " 'Art' 'Health and Lifestyle' 'Entertainment' 'Fashion' 'Entertainment'\n",
            " 'Food' 'Art' 'Art' 'Travel' 'Entertainment' 'Health and Lifestyle' 'Tech'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Fashion' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Travel' 'Travel' 'Health and Lifestyle' 'Travel'\n",
            " 'Art' 'Entertainment' 'Fashion' 'Health and Lifestyle' 'Sports'\n",
            " 'Health and Lifestyle' 'Fashion' 'Health and Lifestyle' 'Fashion'\n",
            " 'Fashion' 'Food' 'Health and Lifestyle' 'Entertainment' 'Food'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Travel'\n",
            " 'Health and Lifestyle' 'Fashion' 'Art' 'Health and Lifestyle' 'Travel'\n",
            " 'Entertainment' 'Travel' 'Health and Lifestyle' 'Entertainment' 'Food'\n",
            " 'Health and Lifestyle' 'Food' 'Food' 'Travel' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Travel' 'Travel' 'Art'\n",
            " 'Health and Lifestyle' 'Travel' 'Travel' 'Travel' 'Food'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Health and Lifestyle' 'Fashion'\n",
            " 'Entertainment' 'Mom and Children' 'Fashion' 'Food' 'Food'\n",
            " 'Health and Lifestyle' 'Food' 'Fashion' 'Art' 'Food' 'Food' 'Tech' 'Tech'\n",
            " 'Travel' 'Entertainment' 'Sports' 'Sports' 'Fashion' 'Fashion' 'Tech'\n",
            " 'Food' 'Tech' 'Art' 'Entertainment' 'Tech' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Art' 'Food' 'Health and Lifestyle' 'Fashion'\n",
            " 'Sports' 'Travel' 'Art' 'Mom and Children' 'Art' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Fashion' 'Food' 'Tech' 'Tech'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Art' 'Health and Lifestyle' 'Food' 'Health and Lifestyle' 'Tech' 'Tech'\n",
            " 'Entertainment' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Food' 'Art' 'Tech' 'Health and Lifestyle' 'Tech'\n",
            " 'Food' 'Food' 'Tech' 'Food' 'Food' 'Tech' 'Health and Lifestyle'\n",
            " 'Fashion' 'Travel' 'Art' 'Art' 'Food' 'Fashion' 'Tech' 'Food'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Art' 'Travel'\n",
            " 'Health and Lifestyle' 'Mom and Children' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Fashion' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Food' 'Mom and Children' 'Food'\n",
            " 'Tech' 'Health and Lifestyle' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Entertainment' 'Food' 'Travel' 'Entertainment' 'Fashion'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Travel' 'Tech' 'Entertainment' 'Fashion' 'Food' 'Fashion'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Food' 'Art' 'Fashion'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Food'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Food' 'Tech' 'Food'\n",
            " 'Mom and Children' 'Entertainment' 'Food' 'Health and Lifestyle'\n",
            " 'Mom and Children' 'Food' 'Tech' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Food' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Fashion' 'Food' 'Tech' 'Entertainment' 'Food' 'Health and Lifestyle'\n",
            " 'Sports' 'Food' 'Entertainment' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Fashion' 'Art' 'Health and Lifestyle' 'Health and Lifestyle' 'Tech'\n",
            " 'Travel' 'Food' 'Entertainment' 'Tech' 'Entertainment' 'Tech' 'Tech'\n",
            " 'Tech' 'Food' 'Health and Lifestyle' 'Tech' 'Health and Lifestyle' 'Tech'\n",
            " 'Tech' 'Health and Lifestyle' 'Entertainment' 'Art'\n",
            " 'Health and Lifestyle' 'Food' 'Entertainment' 'Art' 'Art' 'Travel' 'Food'\n",
            " 'Food' 'Sports' 'Fashion' 'Food' 'Tech' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Food'\n",
            " 'Health and Lifestyle' 'Food' 'Food' 'Travel' 'Art'\n",
            " 'Health and Lifestyle' 'Travel' 'Fashion' 'Mom and Children' 'Travel'\n",
            " 'Food' 'Entertainment' 'Art' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Food' 'Entertainment' 'Art' 'Food' 'Tech'\n",
            " 'Health and Lifestyle' 'Art' 'Tech' 'Gaming' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Food' 'Sports' 'Health and Lifestyle' 'Travel'\n",
            " 'Health and Lifestyle' 'Fashion' 'Art' 'Tech' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Fashion' 'Entertainment' 'Travel' 'Tech' 'Food' 'Travel'\n",
            " 'Health and Lifestyle' 'Food' 'Food' 'Art' 'Tech' 'Entertainment' 'Food'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Fashion' 'Mom and Children'\n",
            " 'Sports' 'Art' 'Health and Lifestyle' 'Health and Lifestyle' 'Tech'\n",
            " 'Health and Lifestyle' 'Food' 'Art' 'Tech' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Art' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Tech' 'Art' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Food' 'Fashion' 'Tech' 'Travel'\n",
            " 'Food' 'Travel' 'Travel' 'Fashion' 'Art' 'Health and Lifestyle' 'Sports'\n",
            " 'Tech' 'Food' 'Health and Lifestyle' 'Art' 'Health and Lifestyle' 'Art'\n",
            " 'Art' 'Entertainment' 'Tech' 'Tech' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Fashion' 'Entertainment' 'Food'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Art' 'Health and Lifestyle' 'Health and Lifestyle' 'Fashion' 'Food'\n",
            " 'Fashion' 'Art' 'Fashion' 'Travel' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Tech' 'Food' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Food' 'Food' 'Health and Lifestyle' 'Art' 'Tech' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Tech' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Food' 'Fashion' 'Art' 'Fashion' 'Fashion' 'Tech'\n",
            " 'Health and Lifestyle' 'Fashion' 'Health and Lifestyle' 'Art'\n",
            " 'Health and Lifestyle' 'Tech' 'Food' 'Fashion' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Food' 'Health and Lifestyle'\n",
            " 'Fashion' 'Health and Lifestyle' 'Entertainment' 'Entertainment'\n",
            " 'Fashion' 'Food' 'Food' 'Tech' 'Travel' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Food' 'Tech' 'Fashion' 'Food' 'Entertainment'\n",
            " 'Entertainment' 'Food' 'Tech' 'Tech' 'Entertainment' 'Fashion' 'Tech'\n",
            " 'Food' 'Health and Lifestyle' 'Tech' 'Health and Lifestyle' 'Food'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Entertainment' 'Entertainment' 'Health and Lifestyle' 'Tech' 'Travel'\n",
            " 'Health and Lifestyle' 'Fashion' 'Fashion' 'Art' 'Entertainment'\n",
            " 'Fashion' 'Tech' 'Travel' 'Fashion' 'Entertainment' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Art' 'Food' 'Health and Lifestyle' 'Tech' 'Tech' 'Tech' 'Tech'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Travel' 'Travel' 'Art'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Fashion' 'Health and Lifestyle'\n",
            " 'Food' 'Fashion' 'Food' 'Entertainment' 'Fashion' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Food' 'Mom and Children'\n",
            " 'Tech' 'Travel' 'Food' 'Tech' 'Tech' 'Health and Lifestyle' 'Tech' 'Art'\n",
            " 'Sports' 'Food' 'Health and Lifestyle' 'Travel' 'Health and Lifestyle'\n",
            " 'Fashion' 'Travel' 'Tech' 'Food' 'Food' 'Entertainment' 'Tech'\n",
            " 'Health and Lifestyle' 'Food' 'Health and Lifestyle' 'Food' 'Art'\n",
            " 'Health and Lifestyle' 'Fashion' 'Tech' 'Health and Lifestyle' 'Fashion'\n",
            " 'Entertainment' 'Travel' 'Tech' 'Mom and Children' 'Health and Lifestyle'\n",
            " 'Travel' 'Entertainment' 'Food' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Tech' 'Travel' 'Food' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Travel' 'Entertainment'\n",
            " 'Food' 'Health and Lifestyle' 'Art' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Food' 'Fashion' 'Health and Lifestyle' 'Fashion'\n",
            " 'Fashion' 'Health and Lifestyle' 'Sports' 'Health and Lifestyle' 'Food'\n",
            " 'Health and Lifestyle' 'Food' 'Fashion' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Entertainment' 'Travel'\n",
            " 'Health and Lifestyle' 'Tech' 'Travel' 'Food' 'Health and Lifestyle'\n",
            " 'Food' 'Food' 'Food' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Sports' 'Food' 'Health and Lifestyle' 'Food'\n",
            " 'Food' 'Art' 'Entertainment' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Tech' 'Fashion' 'Art' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Tech' 'Food' 'Food' 'Entertainment' 'Art'\n",
            " 'Travel' 'Tech' 'Health and Lifestyle' 'Tech' 'Art' 'Tech'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Tech' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Food' 'Travel' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Food' 'Travel' 'Travel' 'Food' 'Health and Lifestyle'\n",
            " 'Tech' 'Tech' 'Fashion' 'Fashion' 'Food' 'Tech' 'Mom and Children'\n",
            " 'Fashion' 'Health and Lifestyle' 'Health and Lifestyle' 'Tech'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Food' 'Tech' 'Travel'\n",
            " 'Health and Lifestyle' 'Tech' 'Health and Lifestyle' 'Food' 'Fashion'\n",
            " 'Health and Lifestyle' 'Food' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Travel' 'Health and Lifestyle' 'Fashion'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Fashion' 'Fashion'\n",
            " 'Travel' 'Tech' 'Health and Lifestyle' 'Health and Lifestyle' 'Tech'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Tech' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Tech' 'Food' 'Tech' 'Health and Lifestyle' 'Food'\n",
            " 'Fashion' 'Gaming' 'Fashion' 'Health and Lifestyle' 'Mom and Children'\n",
            " 'Fashion' 'Health and Lifestyle' 'Food' 'Entertainment' 'Travel' 'Food'\n",
            " 'Travel' 'Health and Lifestyle' 'Tech' 'Mom and Children'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Fashion' 'Health and Lifestyle' 'Art' 'Tech' 'Health and Lifestyle'\n",
            " 'Tech' 'Entertainment' 'Food' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Tech' 'Entertainment' 'Fashion'\n",
            " 'Food' 'Health and Lifestyle' 'Health and Lifestyle' 'Mom and Children'\n",
            " 'Tech' 'Food' 'Art' 'Health and Lifestyle' 'Mom and Children' 'Food'\n",
            " 'Health and Lifestyle' 'Tech' 'Entertainment' 'Fashion' 'Entertainment'\n",
            " 'Tech' 'Fashion' 'Health and Lifestyle' 'Health and Lifestyle' 'Food'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Tech' 'Tech' 'Sports'\n",
            " 'Health and Lifestyle' 'Fashion' 'Fashion' 'Entertainment' 'Art' 'Art'\n",
            " 'Tech' 'Health and Lifestyle' 'Tech' 'Food' 'Art' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Art' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Tech' 'Food' 'Health and Lifestyle' 'Art' 'Entertainment' 'Tech'\n",
            " 'Health and Lifestyle' 'Art' 'Entertainment' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Entertainment' 'Fashion'\n",
            " 'Tech' 'Health and Lifestyle' 'Fashion' 'Tech' 'Health and Lifestyle'\n",
            " 'Tech' 'Travel' 'Fashion' 'Health and Lifestyle' 'Art' 'Entertainment'\n",
            " 'Entertainment' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Health and Lifestyle' 'Food'\n",
            " 'Health and Lifestyle' 'Travel' 'Food' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Tech' 'Entertainment'\n",
            " 'Tech' 'Tech' 'Travel' 'Entertainment' 'Fashion' 'Health and Lifestyle'\n",
            " 'Fashion' 'Health and Lifestyle' 'Entertainment' 'Food'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Health and Lifestyle'\n",
            " 'Travel' 'Health and Lifestyle' 'Health and Lifestyle' 'Travel' 'Travel'\n",
            " 'Sports' 'Health and Lifestyle' 'Health and Lifestyle' 'Tech' 'Art'\n",
            " 'Fashion' 'Travel' 'Food' 'Art' 'Travel' 'Tech' 'Art'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Food' 'Food' 'Tech' 'Food' 'Food'\n",
            " 'Tech' 'Food' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Health and Lifestyle' 'Tech'\n",
            " 'Entertainment' 'Food' 'Entertainment' 'Food' 'Fashion'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Entertainment' 'Tech'\n",
            " 'Health and Lifestyle' 'Food' 'Art' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Food' 'Tech' 'Travel' 'Tech' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Art' 'Tech' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Tech' 'Food' 'Food' 'Tech' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Food' 'Health and Lifestyle' 'Fashion'\n",
            " 'Health and Lifestyle' 'Tech' 'Food' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Food' 'Entertainment' 'Tech' 'Tech'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Art' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Health and Lifestyle' 'Fashion'\n",
            " 'Health and Lifestyle' 'Tech' 'Entertainment' 'Health and Lifestyle'\n",
            " 'Food' 'Art' 'Travel' 'Mom and Children' 'Health and Lifestyle'\n",
            " 'Entertainment' 'Food' 'Art' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Entertainment' 'Food' 'Health and Lifestyle'\n",
            " 'Food' 'Entertainment' 'Health and Lifestyle' 'Entertainment'\n",
            " 'Health and Lifestyle' 'Travel' 'Health and Lifestyle' 'Food' 'Fashion'\n",
            " 'Entertainment' 'Entertainment' 'Health and Lifestyle' 'Food' 'Food'\n",
            " 'Mom and Children' 'Sports' 'Tech' 'Fashion' 'Travel' 'Food' 'Food'\n",
            " 'Fashion' 'Travel' 'Travel' 'Travel' 'Health and Lifestyle'\n",
            " 'Health and Lifestyle' 'Travel' 'Health and Lifestyle' 'Food']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Mapping of strings to integers: {\n",
        "  'Art': 0,\n",
        "  'Entertainment': 1,\n",
        "  'Fashion': 2,\n",
        "  'Food': 3,\n",
        "  'Gaming': 4,\n",
        "  'Health and Lifestyle': 5,\n",
        "  'Mom and Children': 6,\n",
        "  'Sports': 7,\n",
        "  'Tech': 8,\n",
        "  'Travel': 9}\n",
        "\"\"\"\n",
        "print(y_test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeDbli1ngqX8",
        "outputId": "04ebfef2-21cf-48da-d782-d1ffc3ce1097"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 5 8 8 5 9 2 3 3 9 3 8 5 6 8 8 5 5 5 9 1 8 3 5 5 5 5 7 1 9 1 0 7 8 6 3 3\n",
            " 3 1 1 0 5 8 5 1 3 3 2 2 2 0 1 2 5 8 9 5 1 5 5 9 3 2 2 8 5 7 3 7 7 5 0 1 6\n",
            " 3 3 9 1 5 3 8 1 5 5 5 8 5 1 3 5 8 5 5 1 5 5 5 1 1 9 1 9 3 2 5 5 5 1 2 1 8\n",
            " 5 5 3 3 5 5 2 1 1 9 5 8 5 0 3 5 5 5 3 3 5 9 3 1 2 5 9 9 1 6 0 5 5 0 8 3 2\n",
            " 3 2 3 5 5 5 0 5 2 3 2 0 9 5 5 8 5 7 9 8 2 5 1 6 3 1 1 2 0 0 3 1 8 1 0 9 6\n",
            " 1 2 8 5 5 9 0 5 5 0 8 7 2 9 5 5 7 0 2 3 5 9 2 6 0 8 5 5 5 8 2 3 2 5 1 2 2\n",
            " 7 3 1 5 1 8 3 3 9 1 0 1 0 0 9 2 5 5 2 5 3 8 3 5 9 5 0 3 8 5 4 5 3 8 9 5 3\n",
            " 5 5 6 5 2 3 1 3 5 5 3 8 5 8 5 3 6 1 3 1 1 3 5 5 8 8 1 8 2 5 1 0 8 1 2 6 5\n",
            " 5 8 6 3 1 2 9 8 5 1 3 5 2 9 3 5 3 1 3 8 1 5 9 8 5 0 5 0 3 5 9 3 3 2 5 2 1\n",
            " 2 8 5 5 5 5 8 5 8 5 3 8 3 9 5 0 3 6 5 8 9 2 2 3 1 0 5 5 0 3 5 3 3 5 6 5 5\n",
            " 5 3 5 1 6 5 1 5 5 9 3 2 5 3 3 6 3 2 8 9 8 5 5 0 0 8 2 9 1 3 8 5 5 0 3 3 0\n",
            " 3 1 5 0 8 1 1 9 3 5 3 3 2 5 0 5 1 5 7 5 8 1 9 2 3 5 5 3 1 9 5 3 5 3 1 9 3\n",
            " 6 1 2 5 3 8 3 5 5 5 5 5 5 5 5 5 5 1 2 9 2 5 9 2 9 5 3 8 8 9 9 5 8 5 5 2 2\n",
            " 5 7 3 5 5 3 5 5 5 3 0 0 2 8 5 5 0 0 5 5 0 6 9 1 3 1 1 5 5 5 1 8 3 1 5 2 1\n",
            " 5 3 3 7 0 3 5 8 8 8 8 0 5 5 0 3 3 3 2 2 7 0 2 1 8 1 3 1 8 8 2 5 2 0 3 5 8\n",
            " 5 1 3 8 3 5 8 8 5 5 3 2 1 3 0 0 1 5 2 0 1 0 0 5 8 1 1 1 9 2 5 5 2 3 1 5 8\n",
            " 3 1 1 1 2 7 3 2 0 1 6 3 8 5 6 2 3 0 9 8 5 5 0 8 1 5 5 5 5 9 2 2 1 3 8 8 1\n",
            " 2 3 8 5 8 5 5 3 1 5 2 8 3 2 5 2 1 3 5 8 9 5 0 5 3 1 1 1 0 5 9 3 3 8 5 3 8\n",
            " 9 3 5 5 3 2 9 5 0 0 6 0 3 5 1 8 1 5 8 5 8 3 7 5 1 1 8 5 5 3 5 1 5 3 0 5 2\n",
            " 5 5 5 1 5 6 2 9 5 3 1 5 5 8 0 1 2 5 3 5 3 2 3 9 8 5 5 8 9 3 5 2 9 1 9 3 5\n",
            " 9 8 1 5 8 5 2 7 3 8 3 9 9 7 5 2 0 7 1 5 8 0 5 0 9 6 7 3 9 3 9 1 5 0 5 0 1\n",
            " 9 5 5 5 8 8 5 1 5 0 5 5 8 3 1 2 2 5 3 3 5 8 5 2 0 8 9 3 5 0 5 9 1 2 8 2 8\n",
            " 0 6 8 8 8 1 5 3 8 3 5 0 1 9 5 3 0 9 2 8 1 5 1 2 3 5 3 5 6 8 8 2 1 0 1 5 8\n",
            " 1 1 5 0 2 5 9 2 3 5 2 3 5 8 3 3 0 0 8 0 0 5 5 5 1 5 8 3 1 3 5 3 1 2 7 3 8\n",
            " 5 3 5 5 5 3 9 9 5 3 5 6 8 9 3 5 5 7 1 8 9 2 2 8 8 5 3 1 0 5 2 0 0 5 3 2 2\n",
            " 5 5 9 5 1 5 2 5 2 0 0 1 5 3 2 8 5 3 8 8 2 1 0 5 8 0 5 5 5 1 0 3 3 6 5 8 8\n",
            " 3 8 5 1 1 9 1 0 1 5 5 9 5 5 5 1 2 1 1 9 5 6 5 5 5 5 0 2 9 1 2 0 5 3 3 1 2\n",
            " 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Progressive CVgrid runs:\n",
        "\n",
        "Best hyperparameters found:  {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}\n",
        "Best hyperparameters found:  {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 300}\n",
        "Best hyperparameters found:  {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 300}\n",
        "Best hyperparameters found:  {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 300, 'subsample': 0.7}\n",
        "Best hyperparameters found:  {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 300, 'subsample': 0.7}\n",
        "Best hyperparameters found:  {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 300, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.7}\n",
        "Best hyperparameters found:  {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 300, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.7}\n",
        "Best hyperparameters found:  {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.7}\n",
        "\n",
        "3k:\n",
        "Best hyperparameters found:  {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.7}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4jvtwFgxhVkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}